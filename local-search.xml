<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>2024年振り返り</title>
    <link href="/2025/01/30/2024%E5%B9%B4%E6%8C%AF%E3%82%8A%E8%BF%94%E3%82%8A/"/>
    <url>/2025/01/30/2024%E5%B9%B4%E6%8C%AF%E3%82%8A%E8%BF%94%E3%82%8A/</url>
    
    <content type="html"><![CDATA[<h2 id="2024年振り返り"><a href="#2024年振り返り" class="headerlink" title="2024年振り返り"></a>2024年振り返り</h2><p>1月: 台湾で年越し→日本に帰る→修論を書く<br><img src="https://imgur.com/xLfVC5O.jpg" alt="img"><br>2月: スキー→ハイキュー劇場版→ローマで研究発表<br><img src="https://imgur.com/2UN78T1.jpg"><br><img src="https://imgur.com/aKOaJUX.jpg"><br><img src="https://imgur.com/KTw8Ho9.jpg"><br>3月: 引越し準備→神戸で学会→日本語先生と会う→愛車売却→研究室送別会→卒業式→ビザ更新→東京<br><img src="https://imgur.com/SYDNclz.jpg"><br><img src="https://imgur.com/IuZ6STt.jpg"></p><p>4月: 入社式→研修→東京の友達と会う→中目黒と渋谷で花見→more研修→ストリートダンス再開→wanna manna&amp;横浜→従妹と会う<br><img src="https://imgur.com/DE9Ok3O.jpg"><br><img src="https://imgur.com/m7t68Cz.jpg"></p><p>5月: OSN→同期と遊びに行った→more研修→弟と会う<br><img src="https://imgur.com/G4ScAD0.jpg" alt="img"></p><p>6月: more研修→台湾の友達と会う→kaixinパーティー→台舞線練習会<br>7月: moreダンスレッスン &amp; 台舞線練習会→MCAのOB・OG会<br>8月: more研修→猿島→ハリーポッター(スタジオツアー東京)→箱根&amp;熱海→群馬→榛名山→RacingCafe D’z-garage→moreダンス<br>9月: more研修→more ダンス→AWS CP合格<br><img src="https://imgur.com/eT9HEfM.jpg" alt="img"><br><img src="https://imgur.com/MxaErg4.jpg" alt="img"></p><p><img src="https://imgur.com/nHCecKy.jpg"><br><img src="https://imgur.com/zkRsfym.jpg"><br><img src="https://imgur.com/wO0wdUm.jpg"></p><p><img src="https://imgur.com/IU41CFP.jpg" alt="img"><br><img src="https://imgur.com/vbJUV4Z.jpg" alt="img"><br>10月: 業務スタート→WDC→OSSJ &amp; OCSJ summit参加→台舞線練習会<br><img src="https://imgur.com/7H7JjJi.jpg"></p><p><img src="https://imgur.com/LiWIZT7.jpg"><br><img src="https://imgur.com/drvbfxK.jpg"></p><p>11月: 台舞線練習会→新機能開発→母親と会う<br>12月: Zappライブ→台舞線忘年会→ビザ更新→会社忘年会→台湾帰省<br><img src="https://imgur.com/Xs7WUdI.jpg" alt="img"><br><img src="https://imgur.com/nyNwPo5.jpg" alt="img"></p><h2 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h2><p>2021年石川県へ行く前、「必ず東京に帰ってくる」と自分に誓った。<br>そして2年半ぶりにまた東京に帰って、今回は留学生ではなく、社会人としての帰還。</p><p>日本企業で働くからか、社会人1年目のペースは思ったよりも穏やかで、仕事も少しずつ慣れてきた。そして何より嬉しいのは、帰ってきてまだ1年も経たないうちに、ストリートダンスを再開することをきっかけに、東京で多くの台湾人たちと出会えたこと。</p><p>日本での目標も、気づけば半分ぐらい達成していた。今はR&amp;D部門にいるものの、LLMの発展が加速する中で、ソフトウェアエンジニアの価値が今後どう変わっていくのか、ふと考えてしまう。果たしてこの道を進み続けるべきなのか、それともまた新たな分野へ挑戦すべきなのか。</p><p>2025年は、自分の未来を見つめ直し、新たな人生のプランと目標をじっくり探していく一年にしたい。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>gzip is all you need (gzip + kNN outperforms Transformers)</title>
    <link href="/2023/08/15/gzip-is-all-you-need-gzip-kNN-outperforms-Transformers/"/>
    <url>/2023/08/15/gzip-is-all-you-need-gzip-kNN-outperforms-Transformers/</url>
    
    <content type="html"><![CDATA[<h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><ol><li>Abstract</li><li>Introduction</li><li>Approach</li><li>Experiment</li><li>Result</li><li>Limitation</li><li>Reference</li></ol><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p><a href="https://aclanthology.org/2023.findings-acl.426.pdf">“Low-Resource” Text Classification: A Parameter-Free Classification Method with Compressors (Jiang et al., ACL Findings 2023)</a></p><p>大多數神經網絡對data的需求很高，這種需求隨著模型參數數量的增加而增加。</p><p>在眾多模型中，Deep neural networks（DNN）由於準確率高，因此常被用來進行文本分類。然而，DNN 是計算密集型的，在實踐中使用和優化這些模型以及遷移到Out of distribution (OOD) 的成本非常高。</p><p>針對這一缺陷，這篇論文提出了一種文本分類方法，將 $gzip$ 與 $k$NN 相結合。</p><p>採用這種方法在沒有任何訓練參數的情況下，他們在七個分布內數據集和五個分布外數據集上的實驗表明，使用像 $gzip$ 這樣的簡單壓縮器，他們在七個數據集中的結果有六個與 DNNs 結果相當，並在五個 OOD datasets 上勝過包括 BERT 在內的所有方法。即使在few-shot setting下，方法也大幅超越了所有模型。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>在為DNN提供更輕替代方案的所有努力中，有幾篇論文側重於使用壓縮器進行文本分類。就是將文檔和壓縮器構建的類的語言模型之間的最小化cross entropy指示文檔的類。然而，以前的研究未能與神經網絡的準確度相匹配。</p><p>這篇論文的貢獻如下：</p><ol><li>第一個使用NCD和$k$NN進行主題分類，使能夠使用基於壓縮器的方法在大型數據集上進行全面的實驗。 </li><li>方法在七個分佈數據集中的六個上取得了與非預訓練 DNN 相當的結果。</li><li>在 OOD datasets上，表明我們的方法優於所有方法，包括 BERT 等預訓練模型。</li><li>在few-shot setting of scarce labeled data中表現出色。</li></ol><h2 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h2><p>作者使用Compressor進行壓縮源於兩個直覺：</p><ol><li>Compressors 擅長捕捉規律</li><li>同一類別的對象比不同類別的對象具有具有更多的規律性</li></ol><p>For example:</p><p>下面的 $x_1$ 與 $x_2$ 屬於同一類別，但與 $x_3$ 屬於不同類別。如果我們用 $C(\cdot)$ 來表示壓縮長度，會發現 $C (x_1x_2)$ − $C (x_1)$ &lt; $C (x_1x_3)$ − $C (x_1)$，其中 $C (x_1x_2)$ 表示 $x_1$ 和 $x_2$ 串聯的壓縮長度。</p><p>$x_1$ &#x3D; Japan’s Seiko Epson Corp. has developed a l2-gram flying microbot.<br>$x_2$ &#x3D; The latest tiny flying robot has been unveiled in Japan.<br>$x_3$ &#x3D; Michael Phelps won the gold medal in the individual medley.</p><p><img src="https://imgur.com/uz4edvt.jpg"></p><p>由於柯氏複雜性 (Kolmogorov complexity) 的不可計算性導致了 $E(x,y)$ 不可計算，因而 Li et al. 在 2004 年的論文《The similarity metric》中提出信息距離的歸一化和可計算版本，即（Normalized Compression Distance, NCD），它利用壓縮長度 $C(x)$ 來近似柯氏複雜性 $K(x)$。定義如下：</p><p>$$ NCD(x, y) &#x3D; \frac{C(xy) - \min\{C(x), C(y)\}}{\max\{C(x), C(y)\}} $$</p><p>$x$和$y$是需要比較的兩個檔案的byte大小，$C$指壓縮，$C(xy)$就是將$xy$放進一個壓縮包中，$C(x)$就是單獨壓縮$x$以後壓縮包的byte大小，$C(y)$就是單獨壓縮y以後壓縮包的byte大小。 NCD的原理就是如果兩個二進制檔案非常相似，那麽它們被共同壓縮以後，重疊的部分就會很多，這樣壓縮以後的檔案的byte大小就會越小，假如是兩個完全相同的二進制檔案，那麽它們被壓縮以後的體積應該和單獨壓縮一個這個檔案的體積一樣大。也就是：</p><p>$$Idempotency: C(xx) &#x3D; C(x) $$</p><p>由於主要實驗結果使用 $gzip$ 作為壓縮器，所以這里的 $C(x)$ 表示 $x$ 經過 $gzip$ 壓縮後的長度。<br>$C(xy)$ 是連接 $x$ 和 $y$ 的壓縮長度。有了 NCD 提供的距離矩陣，就可以使用 $k$NN 來進行分類。</p><h3 id="14-lines-of-Python-code-for-implement-this-idea…"><a href="#14-lines-of-Python-code-for-implement-this-idea…" class="headerlink" title="14 lines of Python code for implement this idea…"></a>14 lines of Python code for implement this idea…</h3><p>The inputs are training_set, test_set, both consisting of an array of $(text, label)$<br>tuples, and $k$ as shown below.</p><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs scss"># Python <span class="hljs-selector-tag">Code</span> for Text Classification with gzip<br><br>import gzip<br>import numpy as np<br><br>for (x1, _) in test_set:<br>    <br>    Cx1 = <span class="hljs-built_in">len</span>(gzip.<span class="hljs-built_in">compress</span>(x1.<span class="hljs-built_in">encode</span>()))<br>    distance_from_x1 = []<br>    <br>    for (x2, _) in training set:<br>        <br>        Cx2 = <span class="hljs-built_in">len</span>(gzip.<span class="hljs-built_in">compress</span>(x2.<span class="hljs-built_in">encode</span>()))<br>        x1x2 = <span class="hljs-string">&quot; &quot;</span>.<span class="hljs-built_in">join</span>([x1, x2])<br>        Cx1x2 = <span class="hljs-built_in">len</span>(gzip.<span class="hljs-built_in">compress</span>(x1x2.<span class="hljs-built_in">encode</span>()))<br>        ncd = (Cx1x2 - <span class="hljs-built_in">min</span>(Cx1, Cx2)) / <span class="hljs-built_in">max</span>(Cx1, Cx2)<br>        distance_from_x1.<span class="hljs-built_in">append</span>(ncd)<br>    <br>    sorted_idx = np.<span class="hljs-built_in">argsort</span>(np.<span class="hljs-built_in">array</span>(distance_from_x1))<br>    top_k_class = training_set[sorted_idx[:k], <span class="hljs-number">1</span>]<br>    predict_class = <span class="hljs-built_in">max</span>(<span class="hljs-built_in">set</span>(top_k_class), key=top_k_class.count)<br></code></pre></td></tr></table></figure><h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><p><img src="https://imgur.com/MPQJ2QM.jpg"></p><h3 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h3><p>提案手法的結果與（1）需要訓練的神經網絡方法和（2）直接使用$k$NN的非參數方法進行比較，無論是否對外部數據進行pre-trained。</p><p>還對比了包括其他三種非參數方法：word2vec、SentBERT和TextLength，所有這些都使用了$k$NN。</p><p><img src="https://imgur.com/ozIJ8uZ.jpg"></p><h2 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h2><h3 id="Result-on-in-distribution-Datasets"><a href="#Result-on-in-distribution-Datasets" class="headerlink" title="Result on in-distribution Datasets"></a>Result on in-distribution Datasets</h3><ol><li><p>作者在下面Table 3中七個數據集上訓練所有baseline，結果顯示，$gzip$ 在 AG News、R8 和 R52 上表現得非常好。</p></li><li><p>其中在 AG News 上，fine-tuning BERT 在所有方法中取得了最佳性能，而 gzip 在沒有任何預訓練情況下取得了有競爭力的結果，僅比 BERT 低了 0.007。</p></li><li><p>在 R8 和 R52 上，唯一優於 $gzip$ 的非預訓練神經網絡是 HAN。在 YahooAnswers上，gzip 的準確率比一般神經方法低了約 7%。這可能是由於該數據集上的詞匯量較大，導致壓縮器難以壓縮。</p></li><li><p>gzip 在極大的數據集（例：YahooAnswers）上表現不佳，但在中小型數據集上很有競爭力。</p></li></ol><p><img src="https://imgur.com/9DZTTGy.jpg"></p><p>在下面Table 4 中列出了所有baseline models的測試準確率平均值（TextLength 除外）。結果顯示，gzip 在除 YahooAnswers 之外的所有數據集上都高於或接近平均值。</p><p><img src="https://imgur.com/3SiUFFK.jpg"></p><h3 id="Result-on-out-of-distribution-Datasets"><a href="#Result-on-out-of-distribution-Datasets" class="headerlink" title="Result on out-of-distribution Datasets"></a>Result on out-of-distribution Datasets</h3><ol><li><p>在下面Table 5中, 無需任何pre-train或fine-tuning，gzip 在所有數據集上優於 BERT 和 mBERT。</p></li><li><p>結果表明，gzip 在 OOD 數據集上優於預訓練和非預訓練深度學習方法，表示該方法在數據集分布方面具有通用性。</p></li></ol><p><img src="https://imgur.com/Lhako11.jpg"></p><h3 id="Few-Shot-Learning"><a href="#Few-Shot-Learning" class="headerlink" title="Few-Shot Learning"></a>Few-Shot Learning</h3><p>作者還在少樣本設置下比較 $gzip$ 與深度學習方法，並在 AG News、DBpedia 和 SogouNews 上對非預訓練和預訓練深度神經網絡進行實驗。</p><p>結果如下圖 2 所示，在三個數據集上，$gzip$ 的性能優於設置為 5、10、50 的非預訓練模型。當 shot 少至數量 $n$ &#x3D; 5 時，$gzip$ 的性能大幅優於非預訓練模型。其中在 AG News 5-shot 設置下，$gzip$ 的準確率比 fastText 高出了 115%。</p><p>此外，在 100-shot 設置下，gzip 在 AG News 和 SogouNews 上的表現也優於非預訓練模型，但在 DBpedia 上的表現稍差。</p><p><img src="https://imgur.com/cNpRPnS.jpg"></p><p>作者進一步在五個OOD數據集上研究了 5-shot 設置下，$gzip$ 與 DNN 方法的比較結果。結果顯示，$gzip$ 大大優於所有深度學習方法。在相應的dataset，該方法比 BERT 準確率分別增加了 91%、40%、59%、58% 和 194%，比 mBERT 準確率分別增加了 100%、67%、40%、12% 和 130%。</p><p><img src="https://imgur.com/jb96Ion.jpg"></p><h2 id="Limitation"><a href="#Limitation" class="headerlink" title="Limitation"></a>Limitation</h2><p>由於$k$NN的 computation complexity 為 $O(n^2)$，當數據集的大小變得非常大時，速度成為這個方法的限制之一。 但可以透過Multi-threads 和 multi-processes 可以大大提高速度。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="https://github.com/bazingagin/npc_gzip">https://github.com/bazingagin/npc_gzip</a></li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>Natural Language Processing</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Breadth-First Search (using Python)</title>
    <link href="/2023/07/09/Breadth-First-Search/"/>
    <url>/2023/07/09/Breadth-First-Search/</url>
    
    <content type="html"><![CDATA[<h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><ol><li>Steps</li><li>Code</li><li>Time Complexity</li><li>Conclusion</li><li>Reference</li></ol><h2 id="Steps"><a href="#Steps" class="headerlink" title="Steps"></a>Steps</h2><ol><li>Add the vertex to start the breadth-first search to the empty queue. Mark that vertex visited.</li><li>Extract a vertex from the queue and add its neighbors to the queue if that isn’t marked visited.</li><li>Repeat step 2 until the queue is empty.</li></ol><h4 id="Add-the-vertex-to-start-the-breadth-first-search-to-the-empty-queue-Mark-that-vertex-visited-從A開始"><a href="#Add-the-vertex-to-start-the-breadth-first-search-to-the-empty-queue-Mark-that-vertex-visited-從A開始" class="headerlink" title="Add the vertex to start the breadth-first search to the empty queue. Mark that vertex visited.(從A開始)"></a>Add the vertex to start the breadth-first search to the empty queue. Mark that vertex visited.(從A開始)</h4><p><img src="https://imgur.com/6xAY9FT.jpg"></p><h4 id="Extract-a-vertex-from-the-queue-and-add-its-neighbors-to-the-queue-if-that-isn’t-marked-visited"><a href="#Extract-a-vertex-from-the-queue-and-add-its-neighbors-to-the-queue-if-that-isn’t-marked-visited" class="headerlink" title="Extract a vertex from the queue and add its neighbors to the queue if that isn’t marked visited."></a>Extract a vertex from the queue and add its neighbors to the queue if that isn’t marked visited.</h4><p><img src="https://imgur.com/bAZPV8V.jpg"></p><h4 id="We-mark-the-vertices-B-and-C-as-visited-because-we-added-these-to-the-queue"><a href="#We-mark-the-vertices-B-and-C-as-visited-because-we-added-these-to-the-queue" class="headerlink" title="We mark the vertices, B and C as visited because we added these to the queue."></a>We mark the vertices, B and C as visited because we added these to the queue.</h4><p><img src="https://imgur.com/AjHasA5.jpg"></p><p><img src="https://imgur.com/7qwO4cC.jpg"></p><h4 id="Then-we-mark-vertices-D-and-E-visited-because-we-added-these-to-the-queue"><a href="#Then-we-mark-vertices-D-and-E-visited-because-we-added-these-to-the-queue" class="headerlink" title="Then we mark vertices D and E visited because we added these to the queue."></a>Then we mark vertices D and E visited because we added these to the queue.</h4><p><img src="https://imgur.com/VepENhs.jpg"></p><h4 id="We-extract-vertex-C-from-the-queue-However-we-don’t-have-any-vertex-added-to-the-queue-because-we’ve-already-visited-all-neighbors-of-vertex-C"><a href="#We-extract-vertex-C-from-the-queue-However-we-don’t-have-any-vertex-added-to-the-queue-because-we’ve-already-visited-all-neighbors-of-vertex-C" class="headerlink" title="We extract vertex C from the queue. However, we don’t have any vertex added to the queue because we’ve already visited all neighbors of vertex C."></a>We extract vertex C from the queue. However, we don’t have any vertex added to the queue because we’ve already visited all neighbors of vertex C.</h4><p><img src="https://imgur.com/0QcY0wt.jpg"></p><h4 id="We-are-going-to-extract-vertices-D-and-E-but-we’ve-also-visited-these-neighbors-before-So-the-queue-is-empty-and-we-finish-to-search-Finally-we’ve-visited-all-reachable-vertices-from-vertex-A-In-other-words-we’ve-marked-all-vertices-visited"><a href="#We-are-going-to-extract-vertices-D-and-E-but-we’ve-also-visited-these-neighbors-before-So-the-queue-is-empty-and-we-finish-to-search-Finally-we’ve-visited-all-reachable-vertices-from-vertex-A-In-other-words-we’ve-marked-all-vertices-visited" class="headerlink" title="We are going to extract vertices D and E, but we’ve also visited these neighbors before. So the queue is empty and we finish to search. Finally, we’ve visited all reachable vertices from vertex A. In other words, we’ve marked all vertices visited."></a>We are going to extract vertices D and E, but we’ve also visited these neighbors before. So the queue is empty and we finish to search. Finally, we’ve visited all reachable vertices from vertex A. In other words, we’ve marked all vertices visited.</h4><h2 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> deque<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">bfs</span>(<span class="hljs-params">graph, vertex</span>):<br>    queue = deque([vertex])<br>    <br>    <span class="hljs-comment"># The level holds distances from the vertex from which we start searching</span><br>    <br>    level = &#123;vertex: <span class="hljs-number">0</span>&#125;<br>     <span class="hljs-comment"># The parent holds the vertex just added as a key </span><br>     <span class="hljs-comment"># and the vertex from which we reach to the vertex just added as a value</span><br>    <br>    parent = &#123;vertex: <span class="hljs-literal">None</span>&#125;<br>    <span class="hljs-keyword">while</span> queue:<br>        v = queue.popleft()<br>        <span class="hljs-comment"># graph[v] returns neighbors of vertex v</span><br>        <span class="hljs-keyword">for</span> n <span class="hljs-keyword">in</span> graph[v]:<br>            <span class="hljs-keyword">if</span> n <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> level:            <br>                queue.append(n)<br>        <br>                <span class="hljs-comment"># we need to increment the i after the for-loop finishes </span><br>                <span class="hljs-comment"># because we need to expand the circle to search</span><br>                <br>                level[n] = level[v] + <span class="hljs-number">1</span><br>                parent[n] = v<br>    <span class="hljs-keyword">return</span> level, parent<br><br><span class="hljs-comment"># Input</span><br>graph = &#123;<br>    <span class="hljs-string">&#x27;A&#x27;</span>: [<span class="hljs-string">&#x27;B&#x27;</span>, <span class="hljs-string">&#x27;C&#x27;</span>],<br>    <span class="hljs-string">&#x27;B&#x27;</span>: [<span class="hljs-string">&#x27;A&#x27;</span>, <span class="hljs-string">&#x27;D&#x27;</span>, <span class="hljs-string">&#x27;E&#x27;</span>],<br>    <span class="hljs-string">&#x27;C&#x27;</span>: [<span class="hljs-string">&#x27;A&#x27;</span>, <span class="hljs-string">&#x27;D&#x27;</span>],<br>    <span class="hljs-string">&#x27;D&#x27;</span>: [<span class="hljs-string">&#x27;B&#x27;</span>, <span class="hljs-string">&#x27;C&#x27;</span>, <span class="hljs-string">&#x27;E&#x27;</span>],<br>    <span class="hljs-string">&#x27;E&#x27;</span>: [<span class="hljs-string">&#x27;B&#x27;</span>, <span class="hljs-string">&#x27;D&#x27;</span>]<br>&#125;<br><br><span class="hljs-built_in">print</span>(bfs(graph,<span class="hljs-string">&#x27;A&#x27;</span>))<br></code></pre></td></tr></table></figure><h2 id="Output"><a href="#Output" class="headerlink" title="Output"></a>Output</h2><figure class="highlight nsis"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs nsis">(&#123;<span class="hljs-string">&#x27;A&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;B&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;C&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;D&#x27;</span>: <span class="hljs-number">2</span>, <span class="hljs-string">&#x27;E&#x27;</span>: <span class="hljs-number">2</span>&#125; <span class="hljs-comment"># level</span><br>&#123;<span class="hljs-string">&#x27;A&#x27;</span>: <span class="hljs-literal">None</span>, <span class="hljs-string">&#x27;B&#x27;</span>: <span class="hljs-string">&#x27;A&#x27;</span>, <span class="hljs-string">&#x27;C&#x27;</span>: <span class="hljs-string">&#x27;A&#x27;</span>, <span class="hljs-string">&#x27;D&#x27;</span>: <span class="hljs-string">&#x27;B&#x27;</span>, <span class="hljs-string">&#x27;E&#x27;</span>: <span class="hljs-string">&#x27;B&#x27;</span>&#125;) <span class="hljs-comment"># parent</span><br></code></pre></td></tr></table></figure><h2 id="Time-Complexity"><a href="#Time-Complexity" class="headerlink" title="Time Complexity"></a>Time Complexity</h2><h4 id="O-V-E"><a href="#O-V-E" class="headerlink" title="O(|V|+|E|)"></a>O(|V|+|E|)</h4><p>vertices: v &#x3D; queue.popleft()<br>E is the set of the edges</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><ol><li><p>The level holds distances from the vertex from which we start searching.</p></li><li><p>The distance from the vertex to itself is 0, of course, we initialize the level above.</p></li><li><p>The parent holds the vertex just added as a key and the vertex from which we reach to the vertex just added as a value.</p></li></ol><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="https://shorturl.at/abzCZ">Understanding the Breadth-First Search with Python</a></li><li><a href="https://youtu.be/s-CYnVz-uh4">MIT OpenCourseWare 6.006 Lecture 13: Breadth-First Search</a></li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>Algorithm</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>System Design - YouTube</title>
    <link href="/2023/04/09/system-design-youtube/"/>
    <url>/2023/04/09/system-design-youtube/</url>
    
    <content type="html"><![CDATA[<h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><ol><li>Clarify the requirements 了解系統需求</li><li>Capacity Estimation 頻寬存儲估計</li><li>System APIs 系統API設計</li><li>High-level System Design 系統系統設計</li><li>Data Storage 數據存儲設計</li><li>Scalability 優化</li></ol><h2 id="Step1-Clarify-the-requirements-了解系統需求"><a href="#Step1-Clarify-the-requirements-了解系統需求" class="headerlink" title="Step1: Clarify the requirements 了解系統需求"></a>Step1: Clarify the requirements 了解系統需求</h2><ul><li>Feature Requirements</li><li>Traffic&#x2F;User size(e.g., Daily Active User) 服務流量的大小</li></ul><p>Nobody expect you design a complete system in 30-45 mins</p><p>Align with interviewers on 2-3 components to focus in the interview</p><h3 id="系統設計面試重點：要和面試官達成一致"><a href="#系統設計面試重點：要和面試官達成一致" class="headerlink" title="系統設計面試重點：要和面試官達成一致"></a>系統設計面試重點：要和面試官達成一致</h3><h2 id="Type-2-Non-Functional-Requirement"><a href="#Type-2-Non-Functional-Requirement" class="headerlink" title="Type 2: Non-Functional Requirement"></a>Type 2: Non-Functional Requirement</h2><p>為了保證 Availability犧牲了 Consistency<br>設計重點：系統面對比較大流量時的Scalability和Low latency</p><ul><li>Consistency<ul><li>Every read receives the most recent write or an error</li><li>Tradeoff with Availability: Eventual consistency（最终一致性）</li></ul></li><li>Availability<ul><li>Every request receives a (non-error) reponse, without the guarantee that it contains the most recent write</li><li>Scalable<ul><li>Performance: low latency(because it is watching video)</li></ul></li></ul></li><li>Partition tolerance (Fault Tolerance) 系統的容錯性<ul><li>The system continues to operate despite an arbitrary number of messages being dropped(or delayed) by the network between nodes</li></ul></li></ul><h2 id="Step2-Capacity-Estimation"><a href="#Step2-Capacity-Estimation" class="headerlink" title="Step2: Capacity Estimation"></a>Step2: Capacity Estimation</h2><p>Why Capacity Estimation?</p><ul><li>Evaluate candiadate’s analytical skills &amp; system sense</li><li>Helpful for identifying system bottlenecks in order to improve system scalability.</li></ul><p><img src="https://i.imgur.com/6yHnqfO.png"></p><p><img src="https://i.imgur.com/MYKbKk2.png"></p><p>Replication(數據的備份)：通常需要在一個數據中心內把數據備份三份<br>為了系統的可用性: 同一個文件會被分佈到不同的數據中心</p><p>這就需要9倍的儲存空間</p><p><img src="https://i.imgur.com/c5IpqfV.png"></p><p>DAU &#x3D; Daily active user</p><h2 id="Step-3-System-APIs"><a href="#Step-3-System-APIs" class="headerlink" title="Step 3: System APIs"></a>Step 3: System APIs</h2><p><img src="https://i.imgur.com/2k2ugpd.png"></p><p><img src="https://i.imgur.com/u2qeGxA.png"></p><p><img src="https://i.imgur.com/zx336yM.png"></p><p>Offset: 影片的TimeCode</p><p>codec: 影片的編碼格式</p><p>resolution:分辨率（主要取決於頻寬的大小,用來優化觀影體驗）</p><h2 id="Step-4-High-level-System-Design"><a href="#Step-4-High-level-System-Design" class="headerlink" title="Step 4: High-level System Design"></a>Step 4: High-level System Design</h2><p><img src="https://i.imgur.com/Nbm5C9K.png"></p><p>Metadata:影片的標題描述等</p><p>影片本身會存到Distributed Media Storage</p><p>上傳的影片需要經過轉碼處理成不同格式和分辨率的視頻 -&gt; 需要異步處理(Using Processing Queue)</p><p>Video Processing Service: 將處理完的影片跟縮略圖存放到文件系統</p><p>同時在metadata數據庫當中更新影片跟縮圖的存放地址</p><p>For lower latency: CDN (push data to the server that is closer to user)</p><p>Video distributing Service:負責將影片和圖片分發到CDN的各個節點上</p><p>Completion Queue: 異步處理, 當處理完之後往這個隊列添加任務</p><p><img src="https://i.imgur.com/mAuqcms.png"></p><ol><li>下載影片然後把一個影片分成小片段</li><li>對影片解碼再編碼（將影片變成不同的格式和分辨率）</li><li>提取影片縮略圖</li><li>用ML算法來做 video content understanding</li></ol><p><img src="https://i.imgur.com/LP4lUhv.png"></p><p>一般熱門的影片會從CDN上stream給用戶<br>冷門的視頻則由原Data Center stream to user</p><h2 id="Scenario-2"><a href="#Scenario-2" class="headerlink" title="Scenario 2"></a>Scenario 2</h2><p><img src="https://i.imgur.com/iDGdoNP.png"></p><p>Video Playback Service: 主要用來負責影片播放</p><p>Host Identify Service: 用來對影片的地址進行查找也就是說給定一個video給定一個user的IP地址,然後給定用戶的設備信息,查找離這個用戶最近的並且儲存有這個影片的CDN的位置</p><p>如果找到了就把位置回傳給用戶,用戶就可以觀看影片了, 沒有找到就從Data center找影片給用戶觀看</p><p>Metadata&#x2F;User:從數據庫直接讀取影片的標題描述等</p><h2 id="Step-5-Data-Storage"><a href="#Step-5-Data-Storage" class="headerlink" title="Step 5: Data Storage"></a>Step 5: Data Storage</h2><p>PK &#x3D; primary key</p><p><img src="https://i.imgur.com/k8KQ0vp.png"></p><p><img src="https://i.imgur.com/4SO73cJ.png"></p><h2 id="Step-6-Scalability"><a href="#Step-6-Scalability" class="headerlink" title="Step 6: Scalability"></a>Step 6: Scalability</h2><p><img src="https://i.imgur.com/dULCFRN.png"></p><p>找出系統瓶頸,然後提出解決方案和優缺點分析</p><p><img src="https://i.imgur.com/bZ2vUyu.png"></p><p>解決方案：把數據進行多份拷貝分發到不同的機器上,這樣多台機器就能serve不同的requests</p><p><img src="https://i.imgur.com/aDegcGf.png"></p><p><img src="https://i.imgur.com/RUupduh.png"><br>常見的方法：使用primary-secondary</p><p>Pros:</p><ul><li>Availability:隨時都可以讀數據而不用被寫操作影響</li></ul><p>Cons:</p><ul><li>用戶不一定能讀到最新的數據（對用戶沒有多大影響）</li></ul><h2 id="Optimization-3-Caching"><a href="#Optimization-3-Caching" class="headerlink" title="Optimization 3: Caching"></a>Optimization 3: Caching</h2><p><img src="https://i.imgur.com/AHZpjio.png"></p><h2 id="Netfilx-Example-Put-Cache-in-ISP"><a href="#Netfilx-Example-Put-Cache-in-ISP" class="headerlink" title="Netfilx-Example(Put Cache in ISP)"></a>Netfilx-Example(Put Cache in ISP)</h2><p><img src="https://i.imgur.com/rmKp5Ki.png"></p><h2 id="CDN"><a href="#CDN" class="headerlink" title="CDN"></a>CDN</h2><p><img src="https://i.imgur.com/202Ecg5.png"></p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://www.youtube.com/c/HuaHuaLeetCode/videos">花花醬 YouTube</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>system design</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
